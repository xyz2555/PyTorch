{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzZKZP5S6wjN",
        "outputId": "11cb7f7a-fc53-402d-8aa2-89bed4fcf53b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.3331, -0.0201, -0.8109], requires_grad=True)\n",
            "tensor([0.6669, 1.9799, 1.1891], grad_fn=<AddBackward0>)\n",
            "tensor([0.8895, 7.8401, 2.8278], grad_fn=<MulBackward0>)\n",
            "tensor([0.8895, 7.8401, 2.8278], grad_fn=<MulBackward0>)\n",
            "tensor([2.6676e-01, 7.9197e+00, 4.7563e-03])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Mengimpor library PyTorch untuk komputasi tensor dan autograd\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "# Membuat tensor berukuran 3 dengan nilai acak dari distribusi normal (mean=0, std=1)\n",
        "# requires_grad=True berarti PyTorch akan melacak semua operasi pada x\n",
        "\n",
        "print(x)\n",
        "# Menampilkan nilai tensor x\n",
        "\n",
        "y = x + 2\n",
        "# Operasi penjumlahan tensor x dengan konstanta 2\n",
        "# Operasi ini dicatat dalam computation graph\n",
        "\n",
        "print(y)\n",
        "# Menampilkan hasil y\n",
        "\n",
        "z = y * y * 2\n",
        "# Menghitung z = 2 * (y^2) untuk setiap elemen\n",
        "# Operasi ini juga dicatat dalam computation graph\n",
        "\n",
        "print(z)\n",
        "# Menampilkan nilai z (masih berupa tensor vektor)\n",
        "\n",
        "# z = z.mean()\n",
        "# (Jika diaktifkan) Mengubah z menjadi skalar dengan menghitung rata-rata\n",
        "# Biasanya diperlukan agar backward() bisa dipanggil tanpa argumen\n",
        "\n",
        "print(z)\n",
        "# Menampilkan kembali z (masih vektor karena mean dikomentari)\n",
        "\n",
        "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
        "# Membuat tensor v sebagai gradien eksternal\n",
        "# Digunakan saat backward pada tensor non-skalar\n",
        "\n",
        "z.backward(v)\n",
        "# Melakukan backpropagation\n",
        "# v berfungsi sebagai gradien awal (∂L/∂z)\n",
        "# Menghitung gradien z terhadap x\n",
        "\n",
        "print(x.grad)\n",
        "# Menampilkan gradien ∂z/∂x yang tersimpan di x.grad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(3, requires_grad=True)\n",
        "# Membuat ulang tensor x baru dengan nilai acak uniform [0,1]\n",
        "# requires_grad=True untuk pelacakan gradien\n",
        "\n",
        "# x.requires_grad_(False)\n",
        "# (Jika diaktifkan) Menonaktifkan pelacakan gradien pada x\n",
        "\n",
        "print(x)\n",
        "# Menampilkan nilai tensor x\n",
        "\n",
        "# y = x.detach()\n",
        "# detach() membuat tensor baru yang terlepas dari computation graph\n",
        "# Gradien tidak akan mengalir kembali ke x\n",
        "\n",
        "# print(y)\n",
        "# Menampilkan tensor y hasil detach\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Konteks ini menonaktifkan autograd sementara\n",
        "  y = x + 2\n",
        "  # Operasi ini tidak dicatat dalam computation graph\n",
        "  print(y)\n",
        "  # Menampilkan y tanpa melibatkan gradien"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGPL9V2P-UhV",
        "outputId": "deedbbd6-8371-41b2-f280-159ded7fffc0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2704, 0.3450, 0.7644], requires_grad=True)\n",
            "tensor([2.2704, 2.3450, 2.7644])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4, requires_grad=True)\n",
        "# Membuat tensor weights berisi 1 sebanyak 4 elemen\n",
        "# Digunakan sebagai parameter (seperti bobot model)\n",
        "\n",
        "for epoch in range(2):\n",
        "  # Loop training sederhana selama 2 epoch\n",
        "\n",
        "  model_output = (weights * 3).sum()\n",
        "  # Operasi \"model\": setiap weight dikali 3 lalu dijumlahkan\n",
        "  # Hasilnya adalah skalar\n",
        "\n",
        "  model_output.backward()\n",
        "  # Menghitung gradien model_output terhadap weights\n",
        "  # Gradien akan terakumulasi di weights.grad\n",
        "\n",
        "  print(weights.grad)\n",
        "  # Menampilkan gradien weights setelah backward\n",
        "\n",
        "  weights.grad.zero_()\n",
        "  # Mengosongkan gradien\n",
        "  # Penting karena PyTorch mengakumulasi gradien secara default\n",
        "\n",
        "# RINGKASAN:\n",
        "# Kode ini menjelaskan konsep inti autograd di PyTorch:\n",
        "# - Cara PyTorch membangun computation graph secara otomatis\n",
        "# - Backpropagation pada tensor skalar dan non-skalar\n",
        "# - Peran gradien eksternal pada backward()\n",
        "# - Penggunaan requires_grad, detach(), dan torch.no_grad()\n",
        "# - Akumulasi gradien dan pentingnya reset gradien saat training model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poNrV38ODjoB",
        "outputId": "f9669513-e01f-4fa7-ec9a-9fd91ce886aa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    }
  ]
}